{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AHpj7hBOzA22"
   },
   "source": [
    "# Yelp reviews sentiment analysis using pytorch\n",
    "\n",
    "The purpose of this notebook is to go through all the basics for an NLP task. The breakdown of the tasks would be as follows:\n",
    "\n",
    "1. **Data Processing**: Process the raw data, convert it to a pandas dataframe, and manipulate the data according to your need and save it to another csv file.\n",
    "2. **Data Vectorization**: The process of converting the text reviews to a vector of intergers using one hot encoding. Deep learning models do not accept any textual inputs rather you need to feed the inputs as intgers or floats.\n",
    "3. **Data vocabulary**: we need to create a vocabulary for an NLP task, because our model can learn only from the words it has seen so far and their position in the text as well. so for this purpose we need to know which word come how many times in a text, where it appear in the text. we store all such information in a python dictionary\n",
    "4. **Data processing in pytorch**: We process the data in pytorch in using torch dataloader by input our dataset, batch_size. It automatically converts the dataset in batches of tensors for us. so we need not to split the dataset in batches separately. It also handles the autograd for us. In short we are missing out:\n",
    "  * Batching the data\n",
    "  * Shuffling the data\n",
    "  * Load the data in parallel using multiprocessing workers.\n",
    "\n",
    "Dataloader provides all these functions.\n",
    "\n",
    "5. **Deep Learning Model**: so far the models that I a working on are:\n",
    "  * Single Layer Perceptron with following params:\n",
    "      * One Linear Layer of Softmax\n",
    "      * Sigmoid activation unit\n",
    "      * Adam optimizer to upgrade the weights of the parameters\n",
    "      * Binary cross entropy loss which deals with models which spits binary outputs.\n",
    "\n",
    "Explore some feed forward networks:\n",
    "  * MLP: [TODO]\n",
    "  * CNN: [TODO]\n",
    "Explore Feedforward and feed backward networks such as:\n",
    "  * BERT:[TODO]\n",
    "  * RNN: [TODO]\n",
    "  * LSTM: [TODO]\n",
    " \n",
    "\n",
    "6. Training, Validation and Testing Loop.\n",
    "7. Hyperparameters Tuning and their understanding.\n",
    "\n",
    "[TODO]:\n",
    "* Seed understanding as currently the output is changing [DONE]\n",
    "* Use cuda in pytorch and re run the model : [DONE]\n",
    "* Add code to store the vectorized data and model files.\n",
    "* prediction of the model [DONE]\n",
    "* other deep learning model implementation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[NOTE]: One of the thing that I have observed so far is:\n",
    "\n",
    "* If I use the very light dataset, then the simple perceptron works really well.\n",
    "* If I use the full dataset of yelp reviews the perceptron overfits and I get accuracy of 100 percent, and I can clearly observe that it is overfitting because when I print top 20 positive words, it spits random garbage.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "colab_type": "code",
    "id": "2B02D088ccbD",
    "outputId": "eabe69b6-66b6-4f54-bece-ca3e0e8ad52f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'PyTorchNLPBook'...\n",
      "remote: Enumerating objects: 159, done.\u001b[K\n",
      "remote: Total 159 (delta 0), reused 0 (delta 0), pack-reused 159\u001b[K\n",
      "Receiving objects: 100% (159/159), 7.91 MiB | 10.37 MiB/s, done.\n",
      "Resolving deltas: 100% (81/81), done.\n",
      "/content/data\n",
      "Trying to fetch /content/data/yelp/raw_train.csv\n",
      "12536it [00:02, 4684.90it/s]\n",
      "Trying to fetch /content/data/yelp/raw_test.csv\n",
      "848it [00:00, 1754.55it/s]\n",
      "Trying to fetch /content/data/yelp/reviews_with_splits_lite.csv\n",
      "1217it [00:00, 2740.37it/s]\n",
      "Trying to fetch /content/data/surnames/surnames.csv\n",
      "6it [00:00, 4644.85it/s]\n",
      "Trying to fetch /content/data/surnames/surnames_with_splits.csv\n",
      "8it [00:00, 4264.13it/s]\n",
      "Trying to fetch /content/data/books/frankenstein.txt\n",
      "14it [00:00, 5123.04it/s]\n",
      "Trying to fetch /content/data/books/frankenstein_with_splits.csv\n",
      "109it [00:00, 8786.33it/s]\n",
      "Trying to fetch /content/data/ag_news/news.csv\n",
      "188it [00:00, 9783.97it/s]\n",
      "Trying to fetch /content/data/ag_news/news_with_splits.csv\n",
      "208it [00:00, 1223.13it/s]\n",
      "Trying to fetch /content/data/nmt/eng-fra.txt\n",
      "292it [00:00, 1809.70it/s]\n",
      "Trying to fetch /content/data/nmt/simplest_eng_fra.csv\n",
      "30it [00:00, 5221.56it/s]\n",
      "/content\n",
      "data  PyTorchNLPBook  sample_data\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/joosthub/PyTorchNLPBook.git\n",
    "! mv PyTorchNLPBook/data .\n",
    "% cd data \n",
    "! ./get-all-data.sh\n",
    "% cd ../\n",
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aqLmrAHZUZw8"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from argparse import Namespace\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8NQDhfmcVCwv"
   },
   "outputs": [],
   "source": [
    "# defining the arguments for the yelp datasets\n",
    "arg = Namespace(\n",
    "    islite = False,\n",
    "    train_csv_lite_with_split = \"data/yelp/reviews_with_splits_lite.csv\",\n",
    "    train_csv = \"data/yelp/raw_train.csv\",\n",
    "    test_csv = \"data/yelp/raw_test.csv\",\n",
    "    train_split_ratio = 0.75,\n",
    "    test_split_ratio = 0.25,\n",
    "    seed = 1330,\n",
    "    output_file_sentiment = \"data/yelp/final_sentiment.csv\",\n",
    "    output_file_rating = \"data/yelp/final_ratings.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "epcIMFC4Wiht"
   },
   "outputs": [],
   "source": [
    "# lets read the raw data stored in args\n",
    "if not arg.islite:\n",
    "  train_csv  = pd.read_csv(arg.train_csv, header=None, names = [\"rating\",\"review\"])\n",
    "  test_csv = pd.read_csv(arg.test_csv, header=None, names=[\"rating\",\"review\"])\n",
    "  train_csv = train_csv[~pd.isnull(train_csv.review)]\n",
    "  test_csv = test_csv[~pd.isnull(test_csv.review)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "BN5o3AxNarRG",
    "outputId": "972abc5d-873f-4edb-8b68-1636c37a685b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Unfortunately, the frustration of being Dr. Go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Been going to Dr. Goldberg for over 10 years. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>I don't know what Dr. Goldberg was like before...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>I'm writing this review to give you a heads up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>All the food is great here. But the best thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                             review\n",
       "0       1  Unfortunately, the frustration of being Dr. Go...\n",
       "1       2  Been going to Dr. Goldberg for over 10 years. ...\n",
       "2       1  I don't know what Dr. Goldberg was like before...\n",
       "3       1  I'm writing this review to give you a heads up...\n",
       "4       2  All the food is great here. But the best thing..."
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see some rows of train\n",
    "train_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HUjU6iTIZH_0"
   },
   "outputs": [],
   "source": [
    "def partition_dataset(train_csv, test_csv):\n",
    "  # creating new train, validation and test sets\n",
    "  by_ratings = collections.defaultdict(list)\n",
    "  for _, row in train_csv.iterrows():\n",
    "    by_ratings[row.rating].append(row.to_dict())\n",
    "  final_list =[]\n",
    "  seed = 1000\n",
    "  np.random.seed(arg.seed)\n",
    "  for _,  item_list in sorted(by_ratings.items()):\n",
    "    np.random.shuffle(item_list)\n",
    "    total_rows = len(item_list)\n",
    "    total_train_required = int(arg.train_split_ratio*total_rows)\n",
    "    total_test_required = int(arg.test_split_ratio*total_rows)\n",
    "    # Give data point a split attribute\n",
    "    for item in item_list[:total_train_required]:\n",
    "        item['split'] = 'train'\n",
    "    for item in item_list[total_train_required:total_train_required+total_test_required]:\n",
    "        item['split'] = 'val'\n",
    "\n",
    "    # Add to final list\n",
    "    final_list.extend(item_list)\n",
    "    for _, row in test_csv.iterrows():\n",
    "      row_dict = row.to_dict()\n",
    "      row_dict['split'] = 'test'\n",
    "      final_list.append(row_dict)\n",
    "\n",
    "    return final_list, pd.DataFrame(final_list)\n",
    "\n",
    "\n",
    "def preprocess_data(text):\n",
    "  if type(text) == float:\n",
    "        print(text)\n",
    "  text = text.lower()\n",
    "  text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "  text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "  return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-u79eiABcw0w"
   },
   "outputs": [],
   "source": [
    "final_list, final_list_df = partition_dataset(train_csv,test_csv)\n",
    "final_list_df.review = final_list_df.review.apply(preprocess_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "H-zqe8z_gPVD",
    "outputId": "983ac35a-a2de-447f-b7a6-3906ce22256d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train    210000\n",
       "val       70000\n",
       "test      38000\n",
       "Name: split, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the split of data in the dataset\n",
    "final_list_df.split.value_counts()\n",
    "#final_list_df.split['train']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cuoR3St3gnLJ"
   },
   "outputs": [],
   "source": [
    "# Change the rating to sentiment\n",
    "# rating of 1 ==> negative sentiment\n",
    "# rating of 2 ===> positive sentiment\n",
    "final_list_sentiment = final_list_df.copy()\n",
    "final_list_rating = final_list_df.copy()\n",
    "final_list_sentiment['rating'] = final_list_sentiment.rating.apply({1: 'negative', 2: 'positive'}.get)\n",
    "#final_list_rating['rating'] = final_list_rating.rating.apply({'negative':1, 'positive':2}.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "dwQFnOroiP3k",
    "outputId": "54e7dd52-9343-42db-c68e-87f7a9848472"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>used to come here a lot then i think the owner...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>got a notice for the preferred customer sale l...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>the burgers here are probably the best burgers...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>i am a road warrior . i am used to eating alon...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>i come to this place whenever i am in town for...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rating                                             review  split\n",
       "0  negative  used to come here a lot then i think the owner...  train\n",
       "1  negative  got a notice for the preferred customer sale l...  train\n",
       "2  negative  the burgers here are probably the best burgers...  train\n",
       "3  negative  i am a road warrior . i am used to eating alon...  train\n",
       "4  negative  i come to this place whenever i am in town for...  train"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the dataset after changing to sentiment\n",
    "final_list_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "CFa2wD-tia65",
    "outputId": "342ed209-44d6-4517-862f-fb9afdb49589"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>used to come here a lot then i think the owner...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>got a notice for the preferred customer sale l...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>the burgers here are probably the best burgers...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>i am a road warrior . i am used to eating alon...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>i come to this place whenever i am in town for...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                             review  split\n",
       "0       1  used to come here a lot then i think the owner...  train\n",
       "1       1  got a notice for the preferred customer sale l...  train\n",
       "2       1  the burgers here are probably the best burgers...  train\n",
       "3       1  i am a road warrior . i am used to eating alon...  train\n",
       "4       1  i come to this place whenever i am in town for...  train"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_list_rating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OxGaJH5Jj1k4"
   },
   "outputs": [],
   "source": [
    "# save the new dataframes to the csv file\n",
    "final_list_rating.to_csv(arg.output_file_rating)\n",
    "final_list_sentiment.to_csv(arg.output_file_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_O4U37PlmzuJ"
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "  \"class to process text and extract the vocab for mapping\"\n",
    "  def __init__(self, token_to_idx = None, add_unk = True, unk_token='<UNK>'):\n",
    "    if token_to_idx is None:\n",
    "      token_to_idx = {}\n",
    "\n",
    "    self._token_to_idx = token_to_idx\n",
    "    self._idx_to_token = {idx:token for token, idx in self._token_to_idx.items()}\n",
    "    self._add_unk = add_unk\n",
    "    self._unk_token = unk_token\n",
    "    self.unk_index = -1\n",
    "    if add_unk:\n",
    "      self.unk_index = self.add_token(unk_token)\n",
    "\n",
    "  def to_serializable(self):\n",
    "    \"returns  a dictionary which can be serializable\"\n",
    "    return {'token_to_idx': self._token_to_idx,\n",
    "            'add_unk': self._add_unk,\n",
    "            'unk_token':self._unk_token}\n",
    "\n",
    "  @classmethod\n",
    "  def from_serializable(cls, contents):\n",
    "    \"instantiate a vocab from serialized dictionary\"\n",
    "    return cls(**contents)\n",
    "\n",
    "  def add_token(self, token):\n",
    "    \"update the mapping dictionary based ont the token\"\n",
    "    if token in self._token_to_idx:\n",
    "      index = self._token_to_idx[token]\n",
    "    else:\n",
    "      index = len(self._token_to_idx)\n",
    "      self._token_to_idx[token] = index\n",
    "      self._idx_to_token[index] = token\n",
    "\n",
    "    return index\n",
    "\n",
    "\n",
    "  def lookup_token(self, token):\n",
    "    \"retrieve the index based on the token from the mapping dictionary\"\n",
    "    # i=1\n",
    "    # for k,v in self._token_to_idx.items():\n",
    "    #   if i==10:\n",
    "    #     break\n",
    "    #   print(k,v)\n",
    "    if self.unk_index >= 0:\n",
    "      return self._token_to_idx.get(token, self.unk_index)\n",
    "    else:\n",
    "      #print(self._token_to_idx)\n",
    "      if token not in self._token_to_idx:\n",
    "        #print(self._token_to_idx.keys())\n",
    "        return 0\n",
    "      return self._token_to_idx[token]\n",
    "\n",
    "  def lookup_index(self, index):\n",
    "    \"retrieve the token based on the index from the mapping dictionary\"\n",
    "    if index not in self._idx_to_token:\n",
    "      raise KeyError(\"The provided index: %d is not in the vocab\"% index)\n",
    "    else:\n",
    "      return self._idx_to_token[index]\n",
    "\n",
    "  \n",
    "\n",
    "  def __str__(self):\n",
    "    return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "    \"length of the vocabulary\"\n",
    "    return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HdA3sDKb8gNR"
   },
   "outputs": [],
   "source": [
    "class ReviewVectorizer(object):\n",
    "  \" Used the vocbulary class to convert the tokens into actual numerical vector\"\n",
    "\n",
    "  def __init__(self, review_vocab, rating_vocab):\n",
    "    \"\"\"\n",
    "    review_vocab :  maps word to integer\n",
    "    rating_vocab : maps class label to integer(\"negative/positive\")\n",
    "    \"\"\"\n",
    "    self.review_vocab = review_vocab\n",
    "    self.rating_vocab = rating_vocab\n",
    "\n",
    "  def vectorize(self, review):\n",
    "    \"vectorize a text review to one hot encoding\"\n",
    "    one_hot = np.zeros(len(self.review_vocab), dtype=np.float32)\n",
    "    for token in review.split(\" \"):\n",
    "      if token not in string.punctuation:\n",
    "        one_hot[self.review_vocab.lookup_token(token)] = 1\n",
    "    return one_hot\n",
    "\n",
    "  @classmethod\n",
    "  def from_dataframe(cls, review_df, cutoff=25):\n",
    "    \"instantiate a vector for reviews directly from dataset dataframe\"\n",
    "    review_vocab = Vocabulary(add_unk=True)\n",
    "    rating_vocab = Vocabulary(add_unk=False)\n",
    "    for rating in sorted(set(review_df.rating)):\n",
    "      rating_vocab.add_token(rating)\n",
    "    \n",
    "    # add top words if count > provided counts\n",
    "    word_count = collections.Counter()\n",
    "    for review in review_df.review:\n",
    "      for word in review.split(\" \"):\n",
    "        if word not in string.punctuation:\n",
    "          word_count[word] += 1\n",
    "\n",
    "\n",
    "    for word, count in word_count.items():\n",
    "      if count > cutoff:\n",
    "        review_vocab.add_token(word)\n",
    "    return cls(review_vocab, rating_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "      review_vocab = Vocabulary.from_serializable(contents['review_vocab'])\n",
    "      rating_vocab = Vocabulary.from_serializable(contents['rating_vocab'])\n",
    "      return cls(review_vocab=review_vocab, rating_vocab=rating_vocab)\n",
    "\n",
    "\n",
    "    def to_serializable(self):\n",
    "      return {'review_vocab' : self.review_vocab.to_serializable(),\n",
    "              'rating_vocab' : self.rating_vocab.to_serializable()         \n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4AJ6DwIrkk94"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, review_df, vectorizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            review_df (pandas.DataFrame): the dataset\n",
    "            vectorizer (ReviewVectorizer): vectorizer instantiated from dataset\n",
    "        \"\"\"\n",
    "        self.review_df = review_df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        self.train_df = self.review_df[self.review_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.review_df[self.review_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.review_df[self.review_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, review_csv):\n",
    "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
    "        \n",
    "        Args:\n",
    "            review_csv (str): location of the dataset\n",
    "        Returns:\n",
    "            an instance of ReviewDataset\n",
    "        \"\"\"\n",
    "        review_df = pd.read_csv(review_csv)\n",
    "        train_review_df = review_df[review_df.split=='train']\n",
    "        return cls(review_df, ReviewVectorizer.from_dataframe(train_review_df))\n",
    "    \n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, review_csv, vectorizer_filepath):\n",
    "        \"\"\"Load dataset and the corresponding vectorizer. \n",
    "        Used in the case in the vectorizer has been cached for re-use\n",
    "        \n",
    "        Args:\n",
    "            review_csv (str): location of the dataset\n",
    "            vectorizer_filepath (str): location of the saved vectorizer\n",
    "        Returns:\n",
    "            an instance of ReviewDataset\n",
    "        \"\"\"\n",
    "        review_df = pd.read_csv(review_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(review_df, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        \"\"\"a static method for loading the vectorizer from file\n",
    "        \n",
    "        Args:\n",
    "            vectorizer_filepath (str): the location of the serialized vectorizer\n",
    "        Returns:\n",
    "            an instance of ReviewVectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return ReviewVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        \"\"\"saves the vectorizer to disk using json\n",
    "        \n",
    "        Args:\n",
    "            vectorizer_filepath (str): the location to save the vectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" returns the vectorizer \"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" selects the splits in the dataset using a column in the dataframe \n",
    "        \n",
    "        Args:\n",
    "            split (str): one of \"train\", \"val\", or \"test\"\n",
    "        \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        \n",
    "        Args:\n",
    "            index (int): the index to the data point \n",
    "        Returns:\n",
    "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        review_vector = \\\n",
    "            self._vectorizer.vectorize(row.review)\n",
    "\n",
    "        rating_index = \\\n",
    "            self._vectorizer.rating_vocab.lookup_token(row.rating)\n",
    "\n",
    "        return {'x_data': review_vector,\n",
    "                'y_target': rating_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FQg2OPzHHXAQ"
   },
   "outputs": [],
   "source": [
    "# Now generate minibatches from the data using pytorch dataloader class\n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will \n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MC_i4rgXJRFK"
   },
   "outputs": [],
   "source": [
    "# A perceptron classifier\n",
    "# Lets define a simple perceptron classifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook\n",
    "class ReviewClassifier(nn.Module):\n",
    "  \"A simple perceptron classifier\"\n",
    "  def __init__(self, num_features):\n",
    "    super(ReviewClassifier, self).__init__()\n",
    "    self.fcl = nn.Linear(in_features = num_features, out_features=1)\n",
    "  \n",
    "  def forward(self, vectorize_review, apply_sigmoid=False):\n",
    "    y_out = self.fcl(vectorize_review).squeeze()\n",
    "    if apply_sigmoid:\n",
    "      y_out = torch.sigmoid(y_out)\n",
    "    return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8hQ6kQKmKqCb"
   },
   "outputs": [],
   "source": [
    "# Lets write  training module to traint the model\n",
    "# first let define some hyperparameters\n",
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace(\n",
    "    # Data and path information\n",
    "    frequency_cutoff=25,\n",
    "    model_state_file='model.pth',\n",
    "    review_csv='data/yelp/reviews_with_splits_lite.csv',\n",
    "    save_dir='data/ch3/yelp/',\n",
    "    vectorizer_file='vectorizer.json',\n",
    "    # No model hyperparameters\n",
    "    # Training hyperparameters\n",
    "    batch_size=512,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=0.01,\n",
    "    num_epochs=5,\n",
    "    seed=1330,\n",
    "    cuda = True,\n",
    "    device = torch.device(\"cuda\")\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I9TkFJz9Lo9D"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "def make_train_state(args):\n",
    "  return {'epoch_index':0,\n",
    "          'train_loss':[],\n",
    "          'train_acc':[],\n",
    "          'val_loss':[],\n",
    "          'val_acc':[],\n",
    "          'test_loss':-1,\n",
    "          'test_acc':1\n",
    "  }\n",
    "\n",
    "train_state = make_train_state(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MxiZGYjJNPDK"
   },
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "  args.cuda= False\n",
    "  args.device = torch.device(\"cpu\")\n",
    "import json\n",
    "dataset = ReviewDataset.load_dataset_and_make_vectorizer(args.review_csv)\n",
    "# dataset.save_vectorizer(args.vectorizer_file)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "classifier = ReviewClassifier(num_features=len(vectorizer.review_vocab))\n",
    "classifier = classifier.to(args.device)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr = args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "PJwjK8uI0_-Q",
    "outputId": "3bea4d32-d31f-412d-e72d-98c91c156714"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(y_pred, y_target):\n",
    "    y_target = y_target.cpu()\n",
    "    y_pred_indices = (torch.sigmoid(y_pred)>0.5).cpu().long()#.max(dim=1)[1]\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100\n",
    "\n",
    "\n",
    "# check if cuda is available or not\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "gsVPNtn5Oxpk",
    "outputId": "c36d726b-8bff-4d19-a5ce-09eb7df78d26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch:  0, Train Loss: 0.3479662631687365, and Train Accuracy: 87.10166529605266\n",
      "Current epoch: 0, Val Loss: 0.2536488119512796 and, Val Accuracy: 91.34521484374999\n",
      "Current epoch:  1, Train Loss: 0.28080788097883524, and Train Accuracy: 89.91570723684214\n",
      "Current epoch: 1, Val Loss: 0.2379146334715187 and, Val Accuracy: 91.650390625\n",
      "Current epoch:  2, Train Loss: 0.24762986267083567, and Train Accuracy: 91.22036047149125\n",
      "Current epoch: 2, Val Loss: 0.22915221036722258 and, Val Accuracy: 91.81315104166667\n",
      "Current epoch:  3, Train Loss: 0.22636587221763638, and Train Accuracy: 91.9864052220395\n",
      "Current epoch: 3, Val Loss: 0.2238097167573869 and, Val Accuracy: 91.9158935546875\n",
      "Current epoch:  4, Train Loss: 0.2109960503091938, and Train Accuracy: 92.54728618421055\n",
      "Current epoch: 4, Val Loss: 0.22060858644545078 and, Val Accuracy: 91.93359375\n"
     ]
    }
   ],
   "source": [
    "# method to train and evaluate\n",
    "for epoch_index in range(args.num_epochs):\n",
    "  train_state['epoch_index']= epoch_index\n",
    "  # iterating over the training dataset\n",
    "  dataset.set_split('train')    \n",
    "  batch_generator = generate_batches(dataset, batch_size=args.batch_size,\n",
    "                                     device = args.device)\n",
    "  running_loss = 0.0\n",
    "  running_acc = 0.0\n",
    "  classifier.train()\n",
    "  for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = classifier(batch_dict['x_data'].float())\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "    loss_batch = loss.item()\n",
    "    running_loss += (loss_batch - running_loss) / (batch_index+1)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_batch- running_acc) / (batch_index+1)\n",
    "  \n",
    "  train_state['train_loss'].append(running_loss)\n",
    "  train_state['train_acc'].append(running_acc)\n",
    "  average_train_loss = np.mean(train_state['train_loss'])\n",
    "  average_train_acc = np.mean(train_state['train_acc'])\n",
    "  print(\"Current epoch:  {}, Train Loss: {}, and Train Accuracy: {}\".format(epoch_index,\n",
    "                                                              average_train_loss,\n",
    "                                                              average_train_acc))\n",
    "\n",
    "  # validation loop for the dataset\n",
    "  # remember while validation we dont need to calculate gradients\n",
    "  # and no need to backpropagate\n",
    "  # so we can directly calculate the accuracy and score using the forward pass\n",
    "  dataset.set_split('val')\n",
    "  batch_generator = generate_batches(dataset, batch_size=args.batch_size,\n",
    "                                    device = args.device)\n",
    "  running_loss =0.0\n",
    "  running_acc = 0.0\n",
    "  classifier.eval()\n",
    "  for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    y_pred = classifier(batch_dict['x_data'].float())\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "    loss_batch = loss.item()\n",
    "    running_loss += (loss_batch - running_loss) / (batch_index +1)\n",
    "    acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_batch - running_acc) / (batch_index+1)\n",
    "    \n",
    "  train_state['val_loss'].append(running_loss)\n",
    "  train_state['val_acc'].append(running_acc)\n",
    "  average_val_loss= np.mean(train_state['val_loss'])\n",
    "  average_val_acc = np.mean(train_state['val_acc'])\n",
    "\n",
    "  print(\"Current epoch: {}, Val Loss: {} and, Val Accuracy: {}\".format(epoch_index,\n",
    "                                                               average_val_loss,\n",
    "                                                              average_val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "YtGKNMqvChMf",
    "outputId": "8f888514-2659-4b2d-a7a1-4727d266eab8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing\n",
      "Test Loss: 0.21806686278432605, and Test Accuracy: 91.72363281250001\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on the held out dataset\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, batch_size=args.batch_size, \n",
    "                                    device= args.device)\n",
    "runnning_loss = 0.0\n",
    "running_acc = 0.0\n",
    "classifier.eval()\n",
    "print(\"testing\")\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "  # print(batch_dict)\n",
    "  y_pred = classifier(batch_dict['x_data'].float())\n",
    "  loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "  loss_batch = loss.item()\n",
    "  running_loss += (loss_batch - running_loss) / (batch_index+1)\n",
    "  acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "  running_acc += (acc_batch- running_acc) / (batch_index+1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc\n",
    "average_test_loss = train_state['test_loss']\n",
    "average_test_acc = train_state['test_acc']\n",
    "print(\"Test Loss: {}, and Test Accuracy: {}\".format(average_test_loss,\n",
    "                                                              average_test_acc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yN5AX8pkl_KJ"
   },
   "outputs": [],
   "source": [
    "# inference and classifying new data points\n",
    "def predict_rating(review, classifier, vectorizer, decision_threshold=0.5):\n",
    "  \"Predict the rating of the review\"\n",
    "  \"decision boundary is the threshold which separates the 2 ratings\"\n",
    "  review = preprocess_data(review)\n",
    "  vectorized_review = torch.tensor(vectorizer.vectorize(review=review)).to(args.device)\n",
    "\n",
    "  result = classifier(vectorized_review.view(1,-1))\n",
    "  probability = torch.sigmoid(result).item()\n",
    "  print(probability)\n",
    "  index=1\n",
    "  if probability < decision_threshold:\n",
    "    index=0\n",
    "  return vectorizer.rating_vocab.lookup_index(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "Ngt42IHo3ZJQ",
    "outputId": "25712ef0-7345-48fe-868e-0cc922de4ad3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7381582856178284\n",
      "That burger was really awesome --> positive\n"
     ]
    }
   ],
   "source": [
    "new_review = \"That burger was really awesome\"\n",
    "prediction = predict_rating(new_review,classifier,vectorizer)\n",
    "print(\"{} --> {}\".format(new_review,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "colab_type": "code",
    "id": "9ekDDmnb3vNi",
    "outputId": "04da49ac-bdc8-413e-9084-df2067e787da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Influential words in Positive Reviews\n",
      "--------------------------------------\n",
      "fantastic\n",
      "delicious\n",
      "ngreat\n",
      "pleasantly\n",
      "amazing\n",
      "excellent\n",
      "great\n",
      "chinatown\n",
      "solid\n",
      "awesome\n",
      "yummy\n",
      "notch\n",
      "bomb\n",
      "yum\n",
      "vegas\n",
      "boba\n",
      "superb\n",
      "perfection\n",
      "perfect\n",
      "deliciousness\n"
     ]
    }
   ],
   "source": [
    "    # weight inspection to see most common positive and negative words\n",
    "fcl_weights = classifier.fcl.weight.detach()[0]\n",
    "_, indices = torch.sort(fcl_weights, dim=0, descending=True)\n",
    "indices = indices.cpu()\n",
    "indices = indices.numpy().tolist()\n",
    "print(\"Influential words in Positive Reviews\")\n",
    "print(\"--------------------------------------\")\n",
    "for i in range(20):\n",
    "  print(vectorizer.review_vocab.lookup_index(indices[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "colab_type": "code",
    "id": "R9yoYylrpZum",
    "outputId": "3f8b5629-0359-453c-fad5-3f03db47ebcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Influential words in Negative Reviews:\n",
      "--------------------------------------\n",
      "mediocre\n",
      "worst\n",
      "meh\n",
      "bland\n",
      "nmaybe\n",
      "slowest\n",
      "terrible\n",
      "horrible\n",
      "rude\n",
      "tasteless\n",
      "awful\n",
      "overpriced\n",
      "unfriendly\n",
      "cancelled\n",
      "poorly\n",
      "disgusting\n",
      "downhill\n",
      "disappointing\n",
      "underwhelmed\n",
      "unacceptable\n"
     ]
    }
   ],
   "source": [
    "# Top 20 negative words\n",
    "print(\"Influential words in Negative Reviews:\")\n",
    "print(\"--------------------------------------\")\n",
    "indices.reverse()\n",
    "for i in range(20):\n",
    "    print(vectorizer.review_vocab.lookup_index(indices[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6CRTsvULwhuv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Pytorch-yelp-review",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
