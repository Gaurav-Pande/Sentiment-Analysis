{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch-yelp-review",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHpj7hBOzA22",
        "colab_type": "text"
      },
      "source": [
        "# Yelp reviews sentiment analysis using pytorch\n",
        "\n",
        "The purpose of this notebook is to go through all the basics for an NLP task. The breakdown of the tasks would be as follows:\n",
        "\n",
        "1. **Data Processing**: Process the raw data, convert it to a pandas dataframe, and manipulate the data according to your need and save it to another csv file.\n",
        "2. **Data Vectorization**: The process of converting the text reviews to a vector of intergers using one hot encoding. Deep learning models do not accept any textual inputs rather you need to feed the inputs as intgers or floats.\n",
        "3. **Data vocabulary**: we need to create a vocabulary for an NLP task, because our model can learn only from the words it has seen so far and their position in the text as well. so for this purpose we need to know which word come how many times in a text, where it appear in the text. we store all such information in a python dictionary\n",
        "4. **Data processing in pytorch**: We process the data in pytorch in using torch dataloader by input our dataset, batch_size. It automatically converts the dataset in batches of tensors for us. so we need not to split the dataset in batches separately. It also handles the autograd for us. In short we are missing out:\n",
        "  * Batching the data\n",
        "  * Shuffling the data\n",
        "  * Load the data in parallel using multiprocessing workers.\n",
        "\n",
        "Dataloader provides all these functions.\n",
        "\n",
        "5. **Deep Learning Model**: so far the models that I a working on are:\n",
        "  * Single Layer Perceptron with following params:\n",
        "      * One Linear Layer of Softmax\n",
        "      * Sigmoid activation unit\n",
        "      * Adam optimizer to upgrade the weights of the parameters\n",
        "      * Binary cross entropy loss which deals with models which spits binary outputs.\n",
        "\n",
        "Explore some feed forward networks:\n",
        "  * MLP: [TODO]\n",
        "  * CNN: [TODO]\n",
        "Explore Feedforward and feed backward networks such as:\n",
        "  * BERT:[TODO]\n",
        "  * RNN: [TODO]\n",
        "  * LSTM: [TODO]\n",
        " \n",
        "\n",
        "6. Training, Validation and Testing Loop.\n",
        "7. Hyperparameters Tuning and their understanding.\n",
        "\n",
        "[TODO]:\n",
        "* Seed understanding as currently the output is changing [DONE]\n",
        "* Use cuda in pytorch and re run the model : [DONE]\n",
        "* Add code to store the vectorized data and model files.\n",
        "* prediction of the model [DONE]\n",
        "* other deep learning model implementation.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "[NOTE]: One of the thing that I have observed so far is:\n",
        "\n",
        "* If I use the very light dataset, then the simple perceptron works really well.\n",
        "* If I use the full dataset of yelp reviews the perceptron overfits and I get accuracy of 100 percent, and I can clearly observe that it is overfitting because when I print top 20 positive words, it spits random garbage.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2B02D088ccbD",
        "colab_type": "code",
        "outputId": "762b9f33-dd09-4d5d-ca76-427b7e898a93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "! git clone https://github.com/Gaurav-Pande/Sentiment-Analysis.git\n",
        "! mv Sentiment-Analysis/data .\n",
        "% cd data \n",
        "! ./get-all-data.sh\n",
        "% cd ../\n",
        "! ls"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Sentiment-Analysis' already exists and is not an empty directory.\n",
            "mv: cannot stat 'Sentiment-Analysis/data': No such file or directory\n",
            "/content/data\n",
            "/content\n",
            "data  sample_data  Sentiment-Analysis\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqLmrAHZUZw8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from argparse import Namespace\n",
        "import string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NQDhfmcVCwv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# defining the arguments for the yelp datasets\n",
        "arg = Namespace(\n",
        "    islite = False,\n",
        "    train_csv_lite_with_split = \"data/yelp/reviews_with_splits_lite.csv\",\n",
        "    train_csv = \"data/yelp/raw_train.csv\",\n",
        "    test_csv = \"data/yelp/raw_test.csv\",\n",
        "    train_split_ratio = 0.75,\n",
        "    test_split_ratio = 0.25,\n",
        "    seed = 1330,\n",
        "    output_file_sentiment = \"data/yelp/final_sentiment.csv\",\n",
        "    output_file_rating = \"data/yelp/final_ratings.csv\"\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epcIMFC4Wiht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lets read the raw data stored in args\n",
        "if not arg.islite:\n",
        "  train_csv  = pd.read_csv(arg.train_csv, header=None, names = [\"rating\",\"review\"])\n",
        "  test_csv = pd.read_csv(arg.test_csv, header=None, names=[\"rating\",\"review\"])\n",
        "  train_csv = train_csv[~pd.isnull(train_csv.review)]\n",
        "  test_csv = test_csv[~pd.isnull(test_csv.review)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BN5o3AxNarRG",
        "colab_type": "code",
        "outputId": "a8bb8e3b-c10f-4644-9384-0fcf33946891",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# see some rows of train\n",
        "train_csv.head()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rating</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Unfortunately, the frustration of being Dr. Go...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Been going to Dr. Goldberg for over 10 years. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>I don't know what Dr. Goldberg was like before...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>I'm writing this review to give you a heads up...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>All the food is great here. But the best thing...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   rating                                             review\n",
              "0       1  Unfortunately, the frustration of being Dr. Go...\n",
              "1       2  Been going to Dr. Goldberg for over 10 years. ...\n",
              "2       1  I don't know what Dr. Goldberg was like before...\n",
              "3       1  I'm writing this review to give you a heads up...\n",
              "4       2  All the food is great here. But the best thing..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUjU6iTIZH_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def partition_dataset(train_csv, test_csv):\n",
        "  # creating new train, validation and test sets\n",
        "  by_ratings = collections.defaultdict(list)\n",
        "  for _, row in train_csv.iterrows():\n",
        "    by_ratings[row.rating].append(row.to_dict())\n",
        "  final_list =[]\n",
        "  seed = 1000\n",
        "  np.random.seed(arg.seed)\n",
        "  for _,  item_list in sorted(by_ratings.items()):\n",
        "    np.random.shuffle(item_list)\n",
        "    total_rows = len(item_list)\n",
        "    total_train_required = int(arg.train_split_ratio*total_rows)\n",
        "    total_test_required = int(arg.test_split_ratio*total_rows)\n",
        "    # Give data point a split attribute\n",
        "    for item in item_list[:total_train_required]:\n",
        "        item['split'] = 'train'\n",
        "    for item in item_list[total_train_required:total_train_required+total_test_required]:\n",
        "        item['split'] = 'val'\n",
        "\n",
        "    # Add to final list\n",
        "    final_list.extend(item_list)\n",
        "    for _, row in test_csv.iterrows():\n",
        "      row_dict = row.to_dict()\n",
        "      row_dict['split'] = 'test'\n",
        "      final_list.append(row_dict)\n",
        "\n",
        "    return final_list, pd.DataFrame(final_list)\n",
        "\n",
        "\n",
        "def preprocess_data(text):\n",
        "  if type(text) == float:\n",
        "        print(text)\n",
        "  text = text.lower()\n",
        "  text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
        "  text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
        "  return text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u79eiABcw0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_list, final_list_df = partition_dataset(train_csv,test_csv)\n",
        "final_list_df.review = final_list_df.review.apply(preprocess_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-zqe8z_gPVD",
        "colab_type": "code",
        "outputId": "6b32a929-b09c-4fca-a4ab-30949f279364",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# see the split of data in the dataset\n",
        "final_list_df.split.value_counts()\n",
        "#final_list_df.split['train']\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "train    210000\n",
              "val       70000\n",
              "test      38000\n",
              "Name: split, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuoR3St3gnLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Change the rating to sentiment\n",
        "# rating of 1 ==> negative sentiment\n",
        "# rating of 2 ===> positive sentiment\n",
        "final_list_sentiment = final_list_df.copy()\n",
        "final_list_rating = final_list_df.copy()\n",
        "final_list_sentiment['rating'] = final_list_sentiment.rating.apply({1: 'negative', 2: 'positive'}.get)\n",
        "#final_list_rating['rating'] = final_list_rating.rating.apply({'negative':1, 'positive':2}.get)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwQFnOroiP3k",
        "colab_type": "code",
        "outputId": "50f609f6-f04e-4926-ca42-5cea1ab8aefb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# see the dataset after changing to sentiment\n",
        "final_list_sentiment.head()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rating</th>\n",
              "      <th>review</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>negative</td>\n",
              "      <td>used to come here a lot then i think the owner...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>negative</td>\n",
              "      <td>got a notice for the preferred customer sale l...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>negative</td>\n",
              "      <td>the burgers here are probably the best burgers...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>negative</td>\n",
              "      <td>i am a road warrior . i am used to eating alon...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>negative</td>\n",
              "      <td>i come to this place whenever i am in town for...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     rating                                             review  split\n",
              "0  negative  used to come here a lot then i think the owner...  train\n",
              "1  negative  got a notice for the preferred customer sale l...  train\n",
              "2  negative  the burgers here are probably the best burgers...  train\n",
              "3  negative  i am a road warrior . i am used to eating alon...  train\n",
              "4  negative  i come to this place whenever i am in town for...  train"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFa2wD-tia65",
        "colab_type": "code",
        "outputId": "06c5ebfe-ff7d-4a07-bf10-040c25686fbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "final_list_rating.head()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rating</th>\n",
              "      <th>review</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>used to come here a lot then i think the owner...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>got a notice for the preferred customer sale l...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>the burgers here are probably the best burgers...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>i am a road warrior . i am used to eating alon...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>i come to this place whenever i am in town for...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   rating                                             review  split\n",
              "0       1  used to come here a lot then i think the owner...  train\n",
              "1       1  got a notice for the preferred customer sale l...  train\n",
              "2       1  the burgers here are probably the best burgers...  train\n",
              "3       1  i am a road warrior . i am used to eating alon...  train\n",
              "4       1  i come to this place whenever i am in town for...  train"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxGaJH5Jj1k4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save the new dataframes to the csv file\n",
        "final_list_rating.to_csv(arg.output_file_rating)\n",
        "final_list_sentiment.to_csv(arg.output_file_sentiment)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O4U37PlmzuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vocabulary(object):\n",
        "  \"class to process text and extract the vocab for mapping\"\n",
        "  def __init__(self, token_to_idx = None, add_unk = True, unk_token='<UNK>'):\n",
        "    if token_to_idx is None:\n",
        "      token_to_idx = {}\n",
        "\n",
        "    self._token_to_idx = token_to_idx\n",
        "    self._idx_to_token = {idx:token for token, idx in self._token_to_idx.items()}\n",
        "    self._add_unk = add_unk\n",
        "    self._unk_token = unk_token\n",
        "    self.unk_index = -1\n",
        "    if add_unk:\n",
        "      self.unk_index = self.add_token(unk_token)\n",
        "\n",
        "  def to_serializable(self):\n",
        "    \"returns  a dictionary which can be serializable\"\n",
        "    return {'token_to_idx': self._token_to_idx,\n",
        "            'add_unk': self._add_unk,\n",
        "            'unk_token':self._unk_token}\n",
        "\n",
        "  @classmethod\n",
        "  def from_serializable(cls, contents):\n",
        "    \"instantiate a vocab from serialized dictionary\"\n",
        "    return cls(**contents)\n",
        "\n",
        "  def add_token(self, token):\n",
        "    \"update the mapping dictionary based ont the token\"\n",
        "    if token in self._token_to_idx:\n",
        "      index = self._token_to_idx[token]\n",
        "    else:\n",
        "      index = len(self._token_to_idx)\n",
        "      self._token_to_idx[token] = index\n",
        "      self._idx_to_token[index] = token\n",
        "\n",
        "    return index\n",
        "\n",
        "\n",
        "  def lookup_token(self, token):\n",
        "    \"retrieve the index based on the token from the mapping dictionary\"\n",
        "    # i=1\n",
        "    # for k,v in self._token_to_idx.items():\n",
        "    #   if i==10:\n",
        "    #     break\n",
        "    #   print(k,v)\n",
        "    if self.unk_index >= 0:\n",
        "      return self._token_to_idx.get(token, self.unk_index)\n",
        "    else:\n",
        "      #print(self._token_to_idx)\n",
        "      if token not in self._token_to_idx:\n",
        "        #print(self._token_to_idx.keys())\n",
        "        return 0\n",
        "      return self._token_to_idx[token]\n",
        "\n",
        "  def lookup_index(self, index):\n",
        "    \"retrieve the token based on the index from the mapping dictionary\"\n",
        "    if index not in self._idx_to_token:\n",
        "      raise KeyError(\"The provided index: %d is not in the vocab\"% index)\n",
        "    else:\n",
        "      return self._idx_to_token[index]\n",
        "\n",
        "  \n",
        "\n",
        "  def __str__(self):\n",
        "    return \"<Vocabulary(size=%d)>\" % len(self)\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    \"length of the vocabulary\"\n",
        "    return len(self._token_to_idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdA3sDKb8gNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReviewVectorizer(object):\n",
        "  \" Used the vocbulary class to convert the tokens into actual numerical vector\"\n",
        "\n",
        "  def __init__(self, review_vocab, rating_vocab):\n",
        "    \"\"\"\n",
        "    review_vocab :  maps word to integer\n",
        "    rating_vocab : maps class label to integer(\"negative/positive\")\n",
        "    \"\"\"\n",
        "    self.review_vocab = review_vocab\n",
        "    self.rating_vocab = rating_vocab\n",
        "\n",
        "  def vectorize(self, review):\n",
        "    \"vectorize a text review to one hot encoding\"\n",
        "    one_hot = np.zeros(len(self.review_vocab), dtype=np.float32)\n",
        "    for token in review.split(\" \"):\n",
        "      if token not in string.punctuation:\n",
        "        one_hot[self.review_vocab.lookup_token(token)] = 1\n",
        "    return one_hot\n",
        "    # for CNNs\n",
        "    # one_hot_matrix = np.zeros(size= (len(self.review_vocab),300), dtype=np.float32 )\n",
        "    # for token in review.split(\" \"):\n",
        "    #   if token not in string.punctuation:\n",
        "\n",
        "\n",
        "\n",
        "  @classmethod\n",
        "  def from_dataframe(cls, review_df, cutoff=25):\n",
        "    \"instantiate a vector for reviews directly from dataset dataframe\"\n",
        "    review_vocab = Vocabulary(add_unk=True)\n",
        "    rating_vocab = Vocabulary(add_unk=False)\n",
        "    for rating in sorted(set(review_df.rating)):\n",
        "      rating_vocab.add_token(rating)\n",
        "    \n",
        "    # add top words if count > provided counts\n",
        "    word_count = collections.Counter()\n",
        "    for review in review_df.review:\n",
        "      for word in review.split(\" \"):\n",
        "        if word not in string.punctuation:\n",
        "          word_count[word] += 1\n",
        "\n",
        "\n",
        "    for word, count in word_count.items():\n",
        "      if count > cutoff:\n",
        "        review_vocab.add_token(word)\n",
        "    return cls(review_vocab, rating_vocab)\n",
        "\n",
        "    @classmethod\n",
        "    def from_serializable(cls, contents):\n",
        "      review_vocab = Vocabulary.from_serializable(contents['review_vocab'])\n",
        "      rating_vocab = Vocabulary.from_serializable(contents['rating_vocab'])\n",
        "      return cls(review_vocab=review_vocab, rating_vocab=rating_vocab)\n",
        "\n",
        "\n",
        "    def to_serializable(self):\n",
        "      return {'review_vocab' : self.review_vocab.to_serializable(),\n",
        "              'rating_vocab' : self.rating_vocab.to_serializable()         \n",
        "      }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AJ6DwIrkk94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, review_df, vectorizer):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            review_df (pandas.DataFrame): the dataset\n",
        "            vectorizer (ReviewVectorizer): vectorizer instantiated from dataset\n",
        "        \"\"\"\n",
        "        self.review_df = review_df\n",
        "        self._vectorizer = vectorizer\n",
        "\n",
        "        self.train_df = self.review_df[self.review_df.split=='train']\n",
        "        self.train_size = len(self.train_df)\n",
        "\n",
        "        self.val_df = self.review_df[self.review_df.split=='val']\n",
        "        self.validation_size = len(self.val_df)\n",
        "\n",
        "        self.test_df = self.review_df[self.review_df.split=='test']\n",
        "        self.test_size = len(self.test_df)\n",
        "\n",
        "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
        "                             'val': (self.val_df, self.validation_size),\n",
        "                             'test': (self.test_df, self.test_size)}\n",
        "\n",
        "        self.set_split('train')\n",
        "\n",
        "    @classmethod\n",
        "    def load_dataset_and_make_vectorizer(cls, review_csv):\n",
        "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
        "        \n",
        "        Args:\n",
        "            review_csv (str): location of the dataset\n",
        "        Returns:\n",
        "            an instance of ReviewDataset\n",
        "        \"\"\"\n",
        "        review_df = pd.read_csv(review_csv)\n",
        "        train_review_df = review_df[review_df.split=='train']\n",
        "        return cls(review_df, ReviewVectorizer.from_dataframe(train_review_df))\n",
        "    \n",
        "    @classmethod\n",
        "    def load_dataset_and_load_vectorizer(cls, review_csv, vectorizer_filepath):\n",
        "        \"\"\"Load dataset and the corresponding vectorizer. \n",
        "        Used in the case in the vectorizer has been cached for re-use\n",
        "        \n",
        "        Args:\n",
        "            review_csv (str): location of the dataset\n",
        "            vectorizer_filepath (str): location of the saved vectorizer\n",
        "        Returns:\n",
        "            an instance of ReviewDataset\n",
        "        \"\"\"\n",
        "        review_df = pd.read_csv(review_csv)\n",
        "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
        "        return cls(review_df, vectorizer)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_vectorizer_only(vectorizer_filepath):\n",
        "        \"\"\"a static method for loading the vectorizer from file\n",
        "        \n",
        "        Args:\n",
        "            vectorizer_filepath (str): the location of the serialized vectorizer\n",
        "        Returns:\n",
        "            an instance of ReviewVectorizer\n",
        "        \"\"\"\n",
        "        with open(vectorizer_filepath) as fp:\n",
        "            return ReviewVectorizer.from_serializable(json.load(fp))\n",
        "\n",
        "    def save_vectorizer(self, vectorizer_filepath):\n",
        "        \"\"\"saves the vectorizer to disk using json\n",
        "        \n",
        "        Args:\n",
        "            vectorizer_filepath (str): the location to save the vectorizer\n",
        "        \"\"\"\n",
        "        with open(vectorizer_filepath, \"w\") as fp:\n",
        "            json.dump(self._vectorizer.to_serializable(), fp)\n",
        "\n",
        "    def get_vectorizer(self):\n",
        "        \"\"\" returns the vectorizer \"\"\"\n",
        "        return self._vectorizer\n",
        "\n",
        "    def set_split(self, split=\"train\"):\n",
        "        \"\"\" selects the splits in the dataset using a column in the dataframe \n",
        "        \n",
        "        Args:\n",
        "            split (str): one of \"train\", \"val\", or \"test\"\n",
        "        \"\"\"\n",
        "        self._target_split = split\n",
        "        self._target_df, self._target_size = self._lookup_dict[split]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._target_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"the primary entry point method for PyTorch datasets\n",
        "        \n",
        "        Args:\n",
        "            index (int): the index to the data point \n",
        "        Returns:\n",
        "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
        "        \"\"\"\n",
        "        row = self._target_df.iloc[index]\n",
        "\n",
        "        review_vector = \\\n",
        "            self._vectorizer.vectorize(row.review)\n",
        "\n",
        "        rating_index = \\\n",
        "            self._vectorizer.rating_vocab.lookup_token(row.rating)\n",
        "\n",
        "        return {'x_data': review_vector,\n",
        "                'y_target': rating_index}\n",
        "\n",
        "    def get_num_batches(self, batch_size):\n",
        "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
        "        \n",
        "        Args:\n",
        "            batch_size (int)\n",
        "        Returns:\n",
        "            number of batches in the dataset\n",
        "        \"\"\"\n",
        "        return len(self) // batch_size  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQg2OPzHHXAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now generate minibatches from the data using pytorch dataloader class\n",
        "def generate_batches(dataset, batch_size, shuffle=True,\n",
        "                     drop_last=True, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    A generator function which wraps the PyTorch DataLoader. It will \n",
        "      ensure each tensor is on the write device location.\n",
        "    \"\"\"\n",
        "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
        "                            shuffle=shuffle, drop_last=drop_last)\n",
        "\n",
        "    for data_dict in dataloader:\n",
        "        out_data_dict = {}\n",
        "        for name, tensor in data_dict.items():\n",
        "            out_data_dict[name] = data_dict[name].to(device)\n",
        "        yield out_data_dict\n",
        "\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j6cJsBYREYV",
        "colab_type": "text"
      },
      "source": [
        "A simple perceptron consist of a single linear layer which does the affine transformation for us and than the output of the Linear layer can be provided to a activation function. We are using the sigmoid activation funtion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MC_i4rgXJRFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A perceptron classifier\n",
        "# Lets define a simple perceptron classifier\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm_notebook\n",
        "class ReviewClassifier(nn.Module):\n",
        "  \"A simple perceptron classifier\"\n",
        "  def __init__(self, num_features):\n",
        "    super(ReviewClassifier, self).__init__()\n",
        "    self.fcl = nn.Linear(in_features = num_features, out_features=1)\n",
        "  \n",
        "  def forward(self, vectorize_review, apply_sigmoid=False):\n",
        "    y_out = self.fcl(vectorize_review).squeeze()\n",
        "    if apply_sigmoid:\n",
        "      y_out = torch.sigmoid(y_out)\n",
        "    return y_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZjiYfmPnwjU",
        "colab_type": "text"
      },
      "source": [
        "A multilayer perceptron consists of many linear layer and a softmax layer, which is optional. Here we are also using dropout for regularization. dropout generally drops some connections in the neural network so that the network does not overfit.\n",
        "\n",
        "Neural networks—especially deep networks with a large number of layers—can create interesting coadaptation between the units. “Coadaptation” is a term from neuroscience, but here it simply refers to a situation in which the connection between two units becomes excessively strong at the expense of connections between other units. This usually results in the model overfitting to the data. By probabilistically dropping connections between units, we can ensure no single unit will always depend on another single unit, leading to robust models. Dropout does not add additional parameters to the model, but requires a single hyperparameter—the “drop probability.”18 This, as you might have guessed, is the probability with which the connections between units are dropped. It is typical to set the drop probability to 0.5\n",
        "\n",
        "Below is the implementation of the MLP with dropout."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXq5ImF1RqqO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiLayerPerceptron(nn.Module):\n",
        "  \"A simple multi Layer perceptron\"\n",
        "  def __init__(self, in_dimension, hidden_dimension, out_dimension):\n",
        "    super(MultiLayerPerceptron, self).__init__()\n",
        "    self.fcl1 = nn.Linear(in_dimension, hidden_dimension)\n",
        "    self.fcl2 = nn.Linear(hidden_dimension, out_dimension)\n",
        "\n",
        "  def forward(self, x_in, apply_softmax= False):\n",
        "    \"The forward pass for the MLP\"\n",
        "    first_layer_output = self.fcl1(x_in)\n",
        "    intermediate = F.relu(first_layer_output)\n",
        "    output = self.fcl2(F.dropout(intermediate, p=0.5))\n",
        "    if apply_softmax:\n",
        "      output = F.softmax(output, dim =1)\n",
        "    return output.squeeze()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZI7LxyL2F1d",
        "colab_type": "text"
      },
      "source": [
        "## CNN or convolutions neural networks:\n",
        "\n",
        "\n",
        "In simple terms you convolve a input matrix with a given convolution matrix (also known as kernel matrix). The convolution operation is the element wise multiplication of kernel matrix with input matrix and then taking sum. This gives us an output matrix which contains important information. This important information can be understood in terms of dimensionality of the CNNs. \n",
        "\n",
        "**Dimensionality:** The convolutions can be 1D, 2D, and 3d (Conv1d,Conv2d and, Conv3d). The 1D convolution are useful for time serie operations where each timestamp has a input feature vector. In thi way the convolution can learn the important features and sequences in our time series data(can be text or anything). 1D CNNs are most commonly used in NLP operations. 2D are used in computer vision or image processing (as it captures information in both directions= width and heigth of the image). 3D convolutions are used in video processing where the 3rd dimension is for frame in each timestep in the video.\n",
        "\n",
        "**Channels:** refers to the feature input dimension. For example for 2D convolutions the channels are 3. As pixels correcponds to a feauture in an image, and accross 3 channels these pixels contains important information, similarly \"pixels\" can be attributed for the words in a text, and channels can be the length of the vocabulary. Pytorch parameters which can be used: in_channel, out_channel\n",
        "\n",
        "**Kernel Size:** It is the dimension or size of the kernel matrix. In terms of nlp we can say the kernel size refers to n-gram, meaning how much words we wanna keep in context or how many words we are going to look when training the model. larger the kernel , the more words the model will look upon.\n",
        "\n",
        "**Stride:** controls the step size in the convolution. If the step size is same size of the kernel then the kernel will not overlap with any input data while convolution operation  and will threfor generate a smaller output matrix. Smaller stride will result in larger output.\n",
        "\n",
        "**Padding:** is done to add extra 0s along the edges of the input. so that it is compatible with the stride while convolving.\n",
        "\n",
        "\n",
        "**Dilation:** controls how the kernel is applied to the input matrix. A dilation of 2 means that the elements of kernels are 2 position away from each other. It basically introduces hole in the kernel matrix.\n",
        "\n",
        "\n",
        "\n",
        "The goal of Model here is to determine a configuration of convolution layers that results in the desired feature vector. All CNN applications are like this: there is an initial set of convolutional layers that extract a feature map that becomes input in some upstream processing. In classification, the upstream processing is almost always the application of a Linear (or fc) layer which does the classification for us.\n",
        "\n",
        "The implementation walk through in this section iterates over the design decisions to construct a feature vector.We begin by constructing an artificial data tensor mirroring the actual data in shape. **The size of the data tensor is going to be three-dimensional**—this is the size of the minibatch of vectorized text data. **If you use a one-hot vector for each character in a sequence of characters, a sequence of one-hot vectors is a matrix, and a minibatch of one-hot matrices is a three-dimensional tensor**. Using the terminology of convolutions, the size of each one-hot vector (usually the size of the vocabulary) is the number of “input channels” and the length of the character sequence is the “width.”\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GNK1S5M2BcA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define a class for the CNNS\n",
        "class ConvolutionNeuralNetwork(nn.Module):\n",
        "  def __init__(self, in_channels, num_channels, output_size):\n",
        "    super(ConvolutionNeuralNetwork, self).__init__()\n",
        "    self.convet = nn.Sequential(\n",
        "        nn.Conv1d(in_channels=in_channels, out_channels=num_channels, kernel_size=1),\n",
        "        nn.ELU(),\n",
        "        nn.Conv1d(in_channels=num_channels, out_channels=num_channels, kernel_size=1, stride=2),\n",
        "        nn.ELU(),\n",
        "        nn.Conv1d(in_channels=num_channels,  out_channels=num_channels, kernel_size=1 , stride=2),\n",
        "        nn.ELU(),\n",
        "        nn.Conv1d(in_channels=num_channels, out_channels=num_channels, kernel_size=1),\n",
        "        nn.ELU()\n",
        "    )\n",
        "    self.fcl = nn.Linear(num_channels, output_size)\n",
        "\n",
        "\n",
        "  def forward(self, x_in, apply_softmax=False):\n",
        "    features = self.convet(x_in)\n",
        "    prediction_vector = self.fcl(features)\n",
        "    if apply_softmax:\n",
        "      prediction_vector = F.softmax(prediction_vector, dim=2)\n",
        "\n",
        "    return prediction_vector\n",
        "\n",
        "        \n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hQ6kQKmKqCb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lets write  training module to traint the model\n",
        "# first let define some hyperparameters\n",
        "from argparse import Namespace\n",
        "\n",
        "args = Namespace(\n",
        "    # Data and path information\n",
        "    frequency_cutoff=25,\n",
        "    model_state_file='model.pth',\n",
        "    review_csv='data/yelp/reviews_with_splits_lite.csv',\n",
        "    save_dir='data/ch3/yelp/',\n",
        "    vectorizer_file='vectorizer.json',\n",
        "    # No model hyperparameters\n",
        "    # Training hyperparameters\n",
        "    batch_size=256,\n",
        "    early_stopping_criteria=5,\n",
        "    learning_rate=0.001,\n",
        "    num_epochs=5,\n",
        "    seed=1330,\n",
        "    cuda = True,\n",
        "    device = torch.device(\"cuda\")\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9TkFJz9Lo9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "def make_train_state(args):\n",
        "  return {'epoch_index':0,\n",
        "          'train_loss':[],\n",
        "          'train_acc':[],\n",
        "          'val_loss':[],\n",
        "          'val_acc':[],\n",
        "          'test_loss':-1,\n",
        "          'test_acc':1\n",
        "  }\n",
        "\n",
        "train_state = make_train_state(args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxiZGYjJNPDK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not torch.cuda.is_available():\n",
        "  args.cuda= False\n",
        "  args.device = torch.device(\"cpu\")\n",
        "import json\n",
        "dataset = ReviewDataset.load_dataset_and_make_vectorizer(args.review_csv)\n",
        "# dataset.save_vectorizer(args.vectorizer_file)\n",
        "vectorizer = dataset.get_vectorizer()\n",
        "# classifier = ReviewClassifier(num_features=len(vectorizer.review_vocab))\n",
        "classifier = MultiLayerPerceptron(in_dimension=len(vectorizer.review_vocab), hidden_dimension=100, out_dimension=1)\n",
        "#classifier = ConvolutionNeuralNetwork(in_channels=len(vectorizer.review_vocab),num_channels=256, output_size=1)\n",
        "classifier = classifier.to(args.device)\n",
        "loss_func = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(classifier.parameters(), lr = args.learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJwjK8uI0_-Q",
        "colab_type": "code",
        "outputId": "435ca9da-9f23-4365-8dd8-d41086fa649e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def compute_accuracy(y_pred, y_target):\n",
        "    y_target = y_target.cpu()\n",
        "    y_pred_indices = (torch.sigmoid(y_pred)>0.5).cpu().long()#.max(dim=1)[1]\n",
        "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
        "    return n_correct / len(y_pred_indices) * 100\n",
        "\n",
        "\n",
        "# check if cuda is available or not\n",
        "\n",
        "np.random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "if args.cuda:\n",
        "    torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    args.cuda = False\n",
        "\n",
        "print(\"Using CUDA: {}\".format(args.cuda))\n",
        "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using CUDA: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsVPNtn5Oxpk",
        "colab_type": "code",
        "outputId": "d11a7a77-d5ba-4f5b-9914-d1d3abfbf2fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# method to train and evaluate\n",
        "for epoch_index in range(args.num_epochs):\n",
        "  train_state['epoch_index']= epoch_index\n",
        "  # iterating over the training dataset\n",
        "  dataset.set_split('train')    \n",
        "  batch_generator = generate_batches(dataset, batch_size=args.batch_size,\n",
        "                                     device = args.device)\n",
        "  running_loss = 0.0\n",
        "  running_acc = 0.0\n",
        "  classifier.train()\n",
        "  for batch_index, batch_dict in enumerate(batch_generator):\n",
        "    optimizer.zero_grad()\n",
        "    # for CNN \n",
        "    #x_in = batch_dict['x_data'][:,:,None]\n",
        "    # For other\n",
        "    x_in = batch_dict['x_data']\n",
        "    y_pred = classifier(x_in.float())\n",
        "    loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
        "    loss_batch = loss.item()\n",
        "    running_loss += (loss_batch - running_loss) / (batch_index+1)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "    running_acc += (acc_batch- running_acc) / (batch_index+1)\n",
        "  \n",
        "  train_state['train_loss'].append(running_loss)\n",
        "  train_state['train_acc'].append(running_acc)\n",
        "  average_train_loss = np.mean(train_state['train_loss'])\n",
        "  average_train_acc = np.mean(train_state['train_acc'])\n",
        "  print(\"Current epoch:  {}, Train Loss: {}, and Train Accuracy: {}\".format(epoch_index,\n",
        "                                                              average_train_loss,\n",
        "                                                              average_train_acc))\n",
        "\n",
        "  # validation loop for the dataset\n",
        "  # remember while validation we dont need to calculate gradients\n",
        "  # and no need to backpropagate\n",
        "  # so we can directly calculate the accuracy and score using the forward pass\n",
        "  dataset.set_split('val')\n",
        "  batch_generator = generate_batches(dataset, batch_size=args.batch_size,\n",
        "                                    device = args.device)\n",
        "  running_loss =0.0\n",
        "  running_acc = 0.0\n",
        "  classifier.eval()\n",
        "  for batch_index, batch_dict in enumerate(batch_generator):\n",
        "    y_pred = classifier(batch_dict['x_data'].float())\n",
        "    loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
        "    loss_batch = loss.item()\n",
        "    running_loss += (loss_batch - running_loss) / (batch_index +1)\n",
        "    acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "    running_acc += (acc_batch - running_acc) / (batch_index+1)\n",
        "    \n",
        "  train_state['val_loss'].append(running_loss)\n",
        "  train_state['val_acc'].append(running_acc)\n",
        "  average_val_loss= np.mean(train_state['val_loss'])\n",
        "  average_val_acc = np.mean(train_state['val_acc'])\n",
        "\n",
        "  print(\"Current epoch: {}, Val Loss: {} and, Val Accuracy: {}\".format(epoch_index,\n",
        "                                                               average_val_loss,\n",
        "                                                              average_val_acc))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current epoch:  0, Train Loss: 0.3343543533990584, and Train Accuracy: 87.21660539215691\n",
            "Current epoch: 0, Val Loss: 0.23227112879976627 and, Val Accuracy: 91.24755859375\n",
            "Current epoch:  1, Train Loss: 0.26143680106288464, and Train Accuracy: 90.06459354575168\n",
            "Current epoch: 1, Val Loss: 0.21898904116824267 and, Val Accuracy: 91.69921875\n",
            "Current epoch:  2, Train Loss: 0.22520814279260717, and Train Accuracy: 91.42667483660132\n",
            "Current epoch: 2, Val Loss: 0.2157269430657228 and, Val Accuracy: 91.77652994791667\n",
            "Current epoch:  3, Train Loss: 0.2012813241385362, and Train Accuracy: 92.33940972222223\n",
            "Current epoch: 3, Val Loss: 0.21489432116504759 and, Val Accuracy: 91.7144775390625\n",
            "Current epoch:  4, Train Loss: 0.18316233113310695, and Train Accuracy: 93.04636437908498\n",
            "Current epoch: 4, Val Loss: 0.21683721505105497 and, Val Accuracy: 91.728515625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtGKNMqvChMf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e182a95d-c510-4417-bdb9-11496e4884e1"
      },
      "source": [
        "# Evaluation on the held out dataset\n",
        "dataset.set_split('test')\n",
        "batch_generator = generate_batches(dataset, batch_size=args.batch_size, \n",
        "                                    device= args.device)\n",
        "runnning_loss = 0.0\n",
        "running_acc = 0.0\n",
        "classifier.eval()\n",
        "print(\"testing\")\n",
        "for batch_index, batch_dict in enumerate(batch_generator):\n",
        "  # print(batch_dict)\n",
        "  y_pred = classifier(batch_dict['x_data'].float())\n",
        "  loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
        "  loss_batch = loss.item()\n",
        "  running_loss += (loss_batch - running_loss) / (batch_index+1)\n",
        "  acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "  running_acc += (acc_batch- running_acc) / (batch_index+1)\n",
        "\n",
        "train_state['test_loss'] = running_loss\n",
        "train_state['test_acc'] = running_acc\n",
        "average_test_loss = train_state['test_loss']\n",
        "average_test_acc = train_state['test_acc']\n",
        "print(\"Test Loss: {}, and Test Accuracy: {}\".format(average_test_loss,\n",
        "                                                              average_test_acc)) "
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testing\n",
            "Test Loss: 0.22730396315455437, and Test Accuracy: 91.61376953125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yN5AX8pkl_KJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# inference and classifying new data points\n",
        "def predict_rating(review, classifier, vectorizer, decision_threshold=0.5):\n",
        "  \"Predict the rating of the review\"\n",
        "  \"decision boundary is the threshold which separates the 2 ratings\"\n",
        "  review = preprocess_data(review)\n",
        "  vectorized_review = torch.tensor(vectorizer.vectorize(review=review)).to(args.device)\n",
        "\n",
        "  result = classifier(vectorized_review.view(1,-1))\n",
        "  probability = torch.sigmoid(result).item()\n",
        "  print(probability)\n",
        "  index=1\n",
        "  if probability < decision_threshold:\n",
        "    index=0\n",
        "  return vectorizer.rating_vocab.lookup_index(index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ngt42IHo3ZJQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "6f567505-4bf2-4762-a2e6-ceca2f219f40"
      },
      "source": [
        "new_review = \"That dinner was not aweful\"\n",
        "prediction = predict_rating(new_review,classifier,vectorizer)\n",
        "print(\"{} --> {}\".format(new_review,prediction))\n",
        "\n",
        "new_review = \"That burger was very bad\"\n",
        "prediction = predict_rating(new_review,classifier,vectorizer)\n",
        "print(\"{} --> {}\".format(new_review,prediction))\n",
        "\n",
        "new_review = \"That burger was very good\"\n",
        "prediction = predict_rating(new_review,classifier,vectorizer)\n",
        "print(\"{} --> {}\".format(new_review,prediction))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.08916141092777252\n",
            "That dinner was not aweful --> negative\n",
            "0.35558733344078064\n",
            "That burger was very bad --> negative\n",
            "0.7931451797485352\n",
            "That burger was very good --> positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ekDDmnb3vNi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "c3571300-f2ca-42d9-a6c2-a3d1f87e861b"
      },
      "source": [
        "# weight inspection to see most common positive and negative words\n",
        "fcl_weights = classifier.fcl1.weight.detach()[0]\n",
        "_, indices = torch.sort(fcl_weights, dim=0, descending=True)\n",
        "indices = indices.cpu()\n",
        "indices = indices.numpy().tolist()\n",
        "print(\"Influential words in Positive Reviews\")\n",
        "print(\"--------------------------------------\")\n",
        "for i in range(20):\n",
        "  print(vectorizer.review_vocab.lookup_index(indices[i]))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Influential words in Positive Reviews\n",
            "--------------------------------------\n",
            "horrible\n",
            "worst\n",
            "bland\n",
            "mediocre\n",
            "awful\n",
            "dried\n",
            "unfriendly\n",
            "slowest\n",
            "nmaybe\n",
            "disappointing\n",
            "meh\n",
            "disgusting\n",
            "poorly\n",
            "unacceptable\n",
            "underwhelmed\n",
            "terrible\n",
            "tasteless\n",
            "eh\n",
            "mills\n",
            "rude\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9yoYylrpZum",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "ca4590cd-19c4-4747-946c-f3469651447d"
      },
      "source": [
        "# Top 20 negative words\n",
        "print(\"Influential words in Negative Reviews:\")\n",
        "print(\"--------------------------------------\")\n",
        "indices.reverse()\n",
        "for i in range(20):\n",
        "    print(vectorizer.review_vocab.lookup_index(indices[i]))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Influential words in Negative Reviews:\n",
            "--------------------------------------\n",
            "fantastic\n",
            "delicious\n",
            "pho\n",
            "amazing\n",
            "vegas\n",
            "perfection\n",
            "pleasantly\n",
            "nyes\n",
            "penny\n",
            "yummy\n",
            "ngreat\n",
            "magic\n",
            "moist\n",
            "divine\n",
            "variety\n",
            "chill\n",
            "melt\n",
            "perfect\n",
            "heaven\n",
            "chinatown\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}